{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82119658-e67f-4a15-a893-9ab75aeeff2f",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "In this notebook, we will explore what gradient-based optimization is.\n",
    "\n",
    "Are you wondering why we're starting here?\n",
    "Here's why: almost every neural network model that is seen out in the wild\n",
    "is trained by some form of gradient descent.\n",
    "If you have a strong intuition for gradient descent,\n",
    "you will have a tremendous chunk of neural network land demystified.\n",
    "\n",
    "Are you ready? Let's get going! ðŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dedc5d-fefe-4136-8fec-2edaf90a6edf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A brief introduction to _optimization_\n",
    "\n",
    "Firstly, we need to understand what we mean by the term _optimization_.\n",
    "Optimization is something we might do in our daily lives,\n",
    "and that's where we can find great analogies.\n",
    "\n",
    "For example, finding driving directions\n",
    "that minimize the amount of time you take to get from A to B,\n",
    "and we would tweak the route we take to achieve that goal.\n",
    "\n",
    "Another example is finding stock picks to maximize financial returns,\n",
    "and we would tweak our stock picks to achieve that goal.\n",
    "\n",
    "These two analogies illuminate the core of optimization:\n",
    "What we mean by optimization really is\n",
    "_finding the minimum or maximum value_ of some function\n",
    "by tweaking some inputs to that function.\n",
    "\n",
    "What other analogies can you think of?\n",
    "Try writing them down for yourself and evaluate whether they follow the same intuition\n",
    "of tweaking inputs/levers to minimize/maximize an outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15a8cb-713f-406e-ad33-55e9d4cdf9ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An anchoring example for the math of optimization\n",
    "\n",
    "In the deep learning world,\n",
    "optimization almost always takes place in the context of some math function.\n",
    "I am always a fan of anchoring examples,\n",
    "so let's take a math function drawn from a family\n",
    "that many of us would be familiar with -\n",
    "the 2nd-degree polynomial.\n",
    "Let's use a specific one, $f(x) = x^2 + 3x - 5$.\n",
    "Plotted in code, it looks something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51251a7-efbe-45d0-8748-4b96602f0983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as np\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return x ** 2 + 3 * x - 5\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "x = np.linspace(-6, 3, 1000)\n",
    "plt.plot(x, f(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038ea34-eac5-4771-9f57-10ab3eac3f3c",
   "metadata": {},
   "source": [
    "### Analytically minimizing $f(x)$\n",
    "\n",
    "$f(x)$ is a smiley-faced function,\n",
    "thus we know that $f(x)$ has a minimum value.\n",
    "For those of you who know how to take derivatives,\n",
    "you can probably work out that the minimum value occurs at $x=-1.5$.\n",
    "As a refresher for those who need it,\n",
    "we start by figuring out what $f'(x)$,\n",
    "the derivative of $f(x)$ w.r.t. $x$, looks like.\n",
    "From introductory calculus, to find the minima or maxima of a function,\n",
    "we know that the value of the derivative at that point is equal to zero,\n",
    "so to find the value of $x$ at which $f'(x)$ is zero, we solve the equation:\n",
    "\n",
    "$$f'(x) = 2x + 3 = 0$$\n",
    "$$x = -1.5$$\n",
    "\n",
    "Just to be doubly-sure that we are finding a minima,\n",
    "we might also want to check the second derivative to make sure it is positive.\n",
    "That's an exercise you may attempt on your own if you desire.\n",
    "\n",
    "The derivative function $f'(x)$ is also known as the _gradient function_.\n",
    "The derivative function tells us how $y$ will change as we change $x$;\n",
    "it always points us in the way that we need to change $x$\n",
    "to increase the value of $f(x)$.\n",
    "You can verify this by plugging in\n",
    "different values of $x$ into the derivative function.\n",
    "At values to the left of the minima, you will get negative numbers\n",
    "while at values to the right of the minima, you will get positive numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562c0dd-bef5-4849-9dbd-343e50d988c7",
   "metadata": {},
   "source": [
    "### Minimizing the polynomial _computationally_\n",
    "\n",
    "While for simple polynomials we can take gradients easily,\n",
    "for complicated math functions,\n",
    "finding an analytical solution to identify the minima or maxima\n",
    "might not be easy!\n",
    "But fret not, that's where computers come into the picture.\n",
    "There is a general purpose way of finding minima or maxima analytically,\n",
    "and that is by gradient descent.\n",
    "\n",
    "The core of gradient descent is as follows:\n",
    "We pick a starting point at will -\n",
    "there are heuristics to pick good starting points,\n",
    "but for now, let's be satisfied with \"at random\".\n",
    "We take little steps in the _negative_ direction of the gradient.\n",
    "At each step, we evaluate the gradient\n",
    "and then take a small step in $x$ that lets us follow\n",
    "the negative direction of the gradient.\n",
    "We repeat this process over and over\n",
    "for a fixed number of steps\n",
    "(which needs to be configured beforehand).\n",
    "This routine is what we call _gradient descent_.\n",
    "\n",
    "<img src=\"../images/01-gradient-descent/gradient-descent-diagram.png\">\n",
    "\n",
    "Those of you who are attuned to the algorithm\n",
    "might see the following key knobs for gradient descent:\n",
    "\n",
    "- Where the initial point is.\n",
    "- How large of a step to take.\n",
    "- How many steps to take.\n",
    "\n",
    "We won't immediately address how to set these,\n",
    "but you should know that those knobs exist.\n",
    "At this point in the course,\n",
    "I'd ask that you accept the default values that we'll provide for you\n",
    "knowing that pragmatically speaking,\n",
    "they'll help you solve the exercises you encounter\n",
    "in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ae1be-f72a-4e66-8592-833650621a6c",
   "metadata": {},
   "source": [
    "### The workhorse powered by _automatic differentiation_\n",
    "\n",
    "Gradient descent is the workhorse of all of modern deep learning.\n",
    "Whenever we fit a deep learning model,\n",
    "we are also minimizing a function -- \n",
    "the _loss function_ -- \n",
    "that tells us how bad our model is.\n",
    "We'll visit the idea of a loss function in the second notebook,\n",
    "so keep that name in your mind for the time being.\n",
    "For now, though, in order to help you get familiar with gradient descent\n",
    "and how to write a program that uses gradient descent in it,\n",
    "we're going to introduce to you an _automatic differentiation system_:\n",
    "a program that can return the derivative function\n",
    "of any function that performs numerical computation.\n",
    "We'll be using JAX,\n",
    "which provides automatic differentiation on top of the NumPy API --\n",
    "an API that many of us in the scientific Python world should be familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e1b66-802f-4eed-8734-fba97edb342e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## JAX's `grad` function\n",
    "\n",
    "Remember, when we are training a model to do some prediction,\n",
    "we need a way of calculating gradient functions so that we can do gradient-based optimization.\n",
    "Calculating the gradient function by hand is sometimes easy,\n",
    "especially if you're competent with algebra and calculus.\n",
    "However, in many cases, calculating the gradient function by hand is tedious and repetitive.\n",
    "As it turns out, though, tedious and repetitive tasks are _exactly_ what computers are great at!\n",
    "\n",
    "This is where JAX comes in.\n",
    "JAX is a Python package developed by friends at Google.\n",
    "You can think of JAX as the NumPy API turned on steroids. \n",
    "JAX ships with an automatic differentiation system,\n",
    "exposed to end-users in a simple function: `grad`.\n",
    "\n",
    "Here's what `grad` does.\n",
    "It takes in a function that returns a scalar output,\n",
    "and returns another _transformed_ function.\n",
    "That newly returned function is the gradient function of the original.\n",
    "The gradient function takes in the same arguments as the original,\n",
    "but will evaluate the gradient of the original function w.r.t. the arguments passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6a501-f36b-42b2-b376-a6b81688200b",
   "metadata": {},
   "source": [
    "### Example use of `grad`\n",
    "\n",
    "Let's see a concrete example of this in action.\n",
    "\n",
    "We have here an $x^2$ polynomial function:\n",
    "\n",
    "$$ f(x) = 3x^2 + 4x - 2 $$\n",
    "\n",
    "Implemented in NumPy, it'll look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46314139-cc1c-4a33-847b-43b43cddff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return 3 * x ** 2 + 4 * x - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316be66-aaab-48ec-b266-0eca0b223b34",
   "metadata": {},
   "source": [
    "Analytically, `func`'s gradient function is going to be:\n",
    "\n",
    "$$ f'(x) = 6x + 4 $$\n",
    "\n",
    "If we wanted, we could write that gradient function in NumPy by hand.\n",
    "However, we're instead going to obtain the gradient function using JAX's `grad` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05554213-be3e-4cef-a457-379d7ed7356e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "func_p = grad(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e7f14-4b06-4284-85f9-479a6f2a13b2",
   "metadata": {},
   "source": [
    "Pictorially, we'd represent the transformation as follows:\n",
    "\n",
    "<img src=\"../images/01-gradient-descent/grad-transform.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f35461-29d0-460e-92c9-f4f3a26e40b5",
   "metadata": {},
   "source": [
    "In the diagram above, the functions that we want to take derivatives of are the nodes,\n",
    "and `grad` is an edge that transforms a function.\n",
    "In other words, `grad` acts as a program transformation device\n",
    "that takes `f` and turns it into `f'`, a.k.a. the first-order derivative of `f`.\n",
    "(You can keep passing the result through `grad` if you desire!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7814bb22-ae6a-4e32-8e1b-b7ce631eb56f",
   "metadata": {},
   "source": [
    "Now, we can evaluate the gradient function w.r.t. any value of $x$.\n",
    "Let's sanity-check that we have it right.\n",
    "If we plug in x = 3, then $f'(x) = 6(3) + 4 = 22$.\n",
    "Calling the same on `func_p`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64191b56-3d6a-4ccd-a796-b18d1ffc6a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "func_p(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c273f-71dc-43f9-bb8d-90dda40cf6d0",
   "metadata": {},
   "source": [
    "In the Jupyter notebook, I would encourage you to hand-calculate the gradient evaluated at various values of $x$ to convince yourself that `grad` does the right thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c9e08-85f4-453e-ace9-c40ae0985fa8",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Seeing how `grad` works isn't the main point though! We want to use the gradient function `func_p` in an optimization routine to _minimize_ the function `func`. Here's how we do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8309bbf-6962-4385-b1ac-31d7cddc0ae2",
   "metadata": {},
   "source": [
    "### Step 1: Initialize `x`\n",
    "\n",
    "Firstly, we start by initializing $x$ at some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d1a12-4fd8-4398-a601-1cb842cd4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e52901-cce7-44da-ada4-f2e13c57c13f",
   "metadata": {},
   "source": [
    "I chose a fixed value to start, but you could have just as well drawn a number from a random number generator instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d4d39-ceca-4a0f-923a-8196972d078b",
   "metadata": {},
   "source": [
    "### Step 2: Take small steps in negative direction of gradient\n",
    "\n",
    "Next, we take steps in the negative direction of the gradient a fixed number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f915f01-1d33-4e3d-840c-6f3e5ca4ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_steps = 300\n",
    "step_size = 5e-3\n",
    "\n",
    "# loop\n",
    "for i in range(num_steps):\n",
    "    x -= func_p(x) * step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a1867-eca6-4ba6-8a6c-9fcd510e08d1",
   "metadata": {},
   "source": [
    "There are two _hyperparameters_ that we can configure here - the number of steps and the step size. Step sizes are canonically smaller than 1.0, and most of the time set by default to 0.005. (There's an Andrej Karpathy tweet about this.) The smaller the step size, the more steps may be needed to reach convergence. The number of steps is the other hyperparameter; the longer we run, the more likely we are to converge on the minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31be86a-e775-4980-a116-0c2d6c7fc768",
   "metadata": {},
   "source": [
    "### Step 3: Verify proximity to stationary point\n",
    "\n",
    "Let's make sure that we have hit someplace near a _stationary point_ in our optimization routine. Stationary points refer to places where the derivative of the function, or the evaluated gradient, is equal to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a9099-e381-4a57-9ab4-3865635f6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d62dc-7d00-4a95-a5ec-2ad198bf57f6",
   "metadata": {},
   "source": [
    "Indeed, we have!\n",
    "And that's a great sign that we're doing the right thing _computationally_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c75ee3-b5be-43de-b007-cd1a65e5078c",
   "metadata": {},
   "source": [
    "### Step 4: Check second derivative\n",
    "\n",
    "Let's also evaluate the second derivative to sanity-check that we have hit a minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b12aa-dfc5-4dba-ba5f-5a951102b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_pp = grad(func_p)\n",
    "func_pp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec46053-37a5-4caf-ba09-03ed2e2dc037",
   "metadata": {},
   "source": [
    "And just like that, we see that the second derivative at our final value of `x` is a positive number,\n",
    "so we know we're in a regime which is a minima!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e43bec-163b-42b8-a628-1f7d653d14d5",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Coming up is a pair of exercises for you to practice what you should have learned here today:\n",
    "\n",
    "1. How to obtain the derivative function of a NumPy/Python function, and\n",
    "2. How to use the evaluated derivative to find the minima of that function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2392b48-b841-4936-a86e-6ea976be777a",
   "metadata": {},
   "source": [
    "### Exercise: Obtaining the derivative function of a Python function\n",
    "\n",
    "For this exercise, \n",
    "we would like you to use `grad` to obtain the first and second derivative functions of a power-4 polynomial\n",
    "and evaluate the derivatives.\n",
    "\n",
    "First off, here is the function in code and in plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0b0c9-a36f-45ae-87ec-1ce5ccfb2bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cubic_polynomial(x):\n",
    "    return 2 * x ** 4 - 3 * x ** 3 - 9 * x ** 2 + 4 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e0498c-90b4-44e0-a42b-5ee8366af1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 4, 1000)\n",
    "y = cubic_polynomial(x)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a61ba-feb7-4851-93d0-67c22f25db10",
   "metadata": {},
   "source": [
    "Now, we would like you to evaluate the gradient at the following values of `x`: `[3.0, 0.5, -2.0]`. (Be sure to pass them in as floats, i.e. `3.0` rather than `3`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dadf27f-43a4-4894-92a1-e046d6c62e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dcubic_polynomial = grad(cubic_polynomial)\n",
    "\n",
    "values = [3.0, 0.5, -2.0]\n",
    "\n",
    "for value in values:\n",
    "    print(dcubic_polynomial(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef4f45-b0f5-4765-8b9e-9e4e039f3f9c",
   "metadata": {},
   "source": [
    "Now, we would like you to evaluate the second derivative at the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159856d8-2031-46fd-9d01-49324c382f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddcubic_polynomial = grad(dcubic_polynomial)\n",
    "\n",
    "for value in values:\n",
    "    print(ddcubic_polynomial(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123cfd0b-c87e-4026-a1fe-9249a96e67aa",
   "metadata": {},
   "source": [
    "Do the values of the second derivative make sense?\n",
    "For the values `3.0` and `-2.0`, they should,\n",
    "because they are within the smiley-faced regime of the function.\n",
    "The value for 0.5 should make sense too,\n",
    "because it's in the frowny-faced regime of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af1631-7f7c-42b8-908a-df21f06c5fef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise: Finding a minima of a Python function using derivatives\n",
    "\n",
    "We are now going to attempt to find one of the minima of that function\n",
    "using what you know about writing gradient descent loops.\n",
    "\n",
    "Using gradient descent, computationally find one of the minima of the power-4 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef66891-1d5a-491f-b6c5-9c67f5f758c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = -5.0\n",
    "\n",
    "for i in range(1000):\n",
    "    x -= dcubic_polynomial(x) * 0.001\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295391f-b6cc-434d-bcab-759b9cace52d",
   "metadata": {},
   "source": [
    "## Multidimensional gradients\n",
    "\n",
    "Thus far, you've seen how to use the `grad` function\n",
    "in the context of functions where we want to obtain the derivative \n",
    "with respect to one parameter.\n",
    "Thinking forward to our machine learning models,\n",
    "we'll need to obtain the derivative of functions with respect to more than one parameter.\n",
    "This is important because even the simplest linear models have two model parameters,\n",
    "while the vast majority of neural networks have hundreds to millions of parameters.\n",
    "\n",
    "So, then, how do we take the derivative of a function that accepts more than one parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706f4d6-07cb-46c2-a5a8-15fc1d1f6e0a",
   "metadata": {},
   "source": [
    "### Derivatives of a 2D function\n",
    "\n",
    "To illustrate, we're going to play a game where we try to find the minima of a 2D function.\n",
    "I will introduce to you the setting here, which will form the basis of your exercise below.\n",
    "\n",
    "You're programming a robot to find the low points in a landscape that has peaks and valleys.\n",
    "This is important because the low points are the likely place where _gold_ could be found.\n",
    "I'm going to provide the math equation that describes your gold field:\n",
    "\n",
    "$$ f(x, y) = sin(x) + sin(y) $$\n",
    "\n",
    "It's possible to write this equation in Python and NumPy in two forms:\n",
    "one where it accepts `(x, y)` coordinates as a tuple,\n",
    "and one where it accepts `x` and `y` independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ee938-1ace-4876-89af-c781a78bc1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def goldfield1(coords):\n",
    "    x, y = coords\n",
    "    return np.arctan(x)**2 + np.power(np.sin(y), 2)\n",
    "\n",
    "\n",
    "def goldfield2(x, y):\n",
    "    return np.arctan(x)**2 + np.power(np.sin(y), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e533cc-4d9c-4fb1-9ff3-d40ef3fcbce4",
   "metadata": {},
   "source": [
    "This is what the math function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037677ec-e8bb-4013-be64-ba53add099d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-5, 5, 0.25)\n",
    "Y = np.arange(-5, 5, 0.25)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = goldfield2(X, Y)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "# ax.set_zlim(-1.01, 1.01)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "# A StrMethodFormatter is used automatically\n",
    "ax.zaxis.set_major_formatter('{x:.02f}')\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a3bc9-e881-4439-99b5-f78a6a9775cc",
   "metadata": {},
   "source": [
    "It should be obvious to you that both `goldfield1` and `goldfield2` are mathematically equivalent,\n",
    "but their function signatures are different, which will make the difference here.\n",
    "By default, `grad` takes the derivative of a function w.r.t. the first argument.\n",
    "Thus, it's easy to take the derivative of `goldfield1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65917f4d-37fd-456a-b3f3-d97026452fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coords = np.array([1.0, 1.0])\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fba37-54ba-4684-8e91-80331234c125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dgoldfield1 = grad(goldfield1)\n",
    "dgoldfield1(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eb872c-7d8d-4b44-a264-0441c0ceb412",
   "metadata": {},
   "source": [
    "However, what do we do for the second function, `goldfield2`? As it turns out, `grad` has an `argnums` keyword argument that tells us with respect to which argument to take the derivative.\n",
    "We can take the derivative w.r.t. both arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1884241-62b4-447f-bec7-f6308b270802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dgoldfield2 = grad(goldfield2, argnums=(0, 1))\n",
    "x, y = coords\n",
    "dgoldfield2(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b971f-c12b-4bbb-9d6d-32db3d4a9730",
   "metadata": {},
   "source": [
    "And we can also take partial derivatives, starting with the derivative w.r.t. `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193cb88-cae2-492f-b08f-6ba9db12f04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dgoldfield2_dx = grad(goldfield2, argnums=0)\n",
    "dgoldfield2_dx(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25775009-01a4-41b0-8c9f-f35b155525e5",
   "metadata": {},
   "source": [
    "and then the derivative w.r.t. `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248cb8dc-fc93-4a4d-8b92-0633426e7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgoldfield2_dy = grad(goldfield2, argnums=1)\n",
    "dgoldfield2_dy(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6bf9e4-93d2-4896-99f6-32efb65150a9",
   "metadata": {},
   "source": [
    "Whether you choose to go with partial or full derivatives\n",
    "will depend on how much complexity you wish to expose\n",
    "to whoever is using the function.\n",
    "In my opinion, `dgoldfield1` is less complex, but is also more restrictive.\n",
    "By contrast, structuring your function in the style of `dgoldfield2`\n",
    "allows you to take a _partial_ derivative with respect to either `x` or `y`,\n",
    "If your particular project requires partial derivatives,\n",
    "you can easily specify, using `argnums` which partial derivative you require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003607f0-6bea-415a-a3d3-e89f564a2d3a",
   "metadata": {},
   "source": [
    "Alrighty, to reinforce these concepts, we've got a bunch of exercises present for you. As always, these examples are present in the notebook for you to review as you attempt the exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151db3e-7b97-4146-a8a9-a40d26a61748",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "We are going to leverage the goldfield function for the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d24be-6e1b-4b85-949a-54edf936246c",
   "metadata": {},
   "source": [
    "### Exercise 1: Find a minima w.r.t. both `x` and `y`\n",
    "\n",
    "As you can see in the plot, there are multiple local minima in the function. We'd like you to find one of them by using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee78653-3bd1-4836-adc3-8c0db374c4a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coords = np.array([0., -2])\n",
    "for i in range(1000):\n",
    "    g = dgoldfield1(coords)\n",
    "    coords -= g * 0.01\n",
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedb8b0-0dac-485d-bd4a-7823d2534ac5",
   "metadata": {},
   "source": [
    "### Exercise 2: Find a minima along one axis, either `x` or `y`\n",
    "\n",
    "Using partial derivatives, find a minima of the goldfield\n",
    "when fixing either `x` or `y` to a particular value.\n",
    "(You get to choose which you wish to fix!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b672c1-fda0-4db1-990d-1913711ecb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This answer assumes that we are finding a minima along the y-axis,\n",
    "# i.e. fixing the value of `x`.\n",
    "\n",
    "x, y = 1.0, 1.0\n",
    "\n",
    "for i in range(1000):\n",
    "    g = dgoldfield2_dy(x, y)\n",
    "    y -= g * 0.01\n",
    "x, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
