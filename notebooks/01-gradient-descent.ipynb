{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82119658-e67f-4a15-a893-9ab75aeeff2f",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "In this notebook, we will explore what gradient-based optimization is.\n",
    "\n",
    "Are you wondering why we're starting here?\n",
    "Here's why: almost every neural network model that is seen out in the wild\n",
    "is trained by some form of gradient descent.\n",
    "If you have a strong intuition for gradient descent,\n",
    "you will have a tremendous chunk of neural network land demystified.\n",
    "\n",
    "Are you ready? Let's get going! ðŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dedc5d-fefe-4136-8fec-2edaf90a6edf",
   "metadata": {},
   "source": [
    "## A brief introduction to _optimization_\n",
    "\n",
    "Firstly, we need to understand what we mean by the term _optimization_.\n",
    "Optimization is something we might do in our daily lives,\n",
    "and that's where we can find great analogies.\n",
    "\n",
    "For example, finding driving directions\n",
    "that minimize the amount of time you take to get from A to B,\n",
    "and we would tweak the route we take to achieve that goal.\n",
    "\n",
    "Another example is finding stock picks to maximize financial returns,\n",
    "and we would tweak our stock picks to achieve that goal.\n",
    "\n",
    "These two analogies illuminate the core of optimization:\n",
    "What we mean by optimization really is\n",
    "_finding the minimum or maximum value_ of some function\n",
    "by tweaking some inputs to that function.\n",
    "\n",
    "What other analogies can you think of?\n",
    "Try writing them down for yourself and evaluate whether they follow the same intuition\n",
    "of tweaking inputs/levers to minimize/maximize an outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15a8cb-713f-406e-ad33-55e9d4cdf9ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An anchoring example for the math of optimization\n",
    "\n",
    "In the deep learning world,\n",
    "optimization almost always takes place in the context of some math function.\n",
    "I am always a fan of anchoring examples,\n",
    "so let's take a math function drawn from a family\n",
    "that many of us would be familiar with -\n",
    "the 2nd-degree polynomial.\n",
    "Let's use a specific one, $f(x) = x^2 + 3x - 5$.\n",
    "Plotted in code, it looks something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51251a7-efbe-45d0-8748-4b96602f0983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return x ** 2 + 3 * x - 5\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "x = np.linspace(-6, 3, 1000)\n",
    "plt.plot(x, f(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038ea34-eac5-4771-9f57-10ab3eac3f3c",
   "metadata": {},
   "source": [
    "### Analytically minimizing $f(x)$\n",
    "\n",
    "$f(x)$ is a smiley-faced function,\n",
    "thus we know that $f(x)$ has a minimum value.\n",
    "For those of you who know how to take derivatives,\n",
    "you can probably work out that the minimum value occurs at $x=-1.5$.\n",
    "As a refresher for those who need it,\n",
    "we start by figuring out what $f'(x)$,\n",
    "the derivative of $f(x)$ w.r.t. $x$, looks like.\n",
    "From introductory calculus, to find the minima or maxima of a function,\n",
    "we know that the value of the derivative at that point is equal to zero,\n",
    "so to find the value of $x$ at which $f'(x)$ is zero, we solve the equation:\n",
    "\n",
    "$$f'(x) = 2x + 3 = 0$$\n",
    "$$x = -1.5$$\n",
    "\n",
    "Just to be doubly-sure that we are finding a minima,\n",
    "we might also want to check the second derivative to make sure it is positive.\n",
    "That's an exercise you may attempt on your own if you desire.\n",
    "\n",
    "The derivative function $f'(x)$ is also known as the _gradient function_.\n",
    "The derivative function tells us how $y$ will change as we change $x$;\n",
    "it always points us in the way that we need to change $x$\n",
    "to increase the value of $f(x)$.\n",
    "You can verify this by plugging in\n",
    "different values of $x$ into the derivative function.\n",
    "At values to the left of the minima, you will get negative numbers\n",
    "while at values to the right of the minima, you will get positive numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562c0dd-bef5-4849-9dbd-343e50d988c7",
   "metadata": {},
   "source": [
    "### Minimizing the polynomial _computationally_\n",
    "\n",
    "While for simple polynomials we can take gradients easily,\n",
    "for complicated math functions,\n",
    "finding an analytical solution to identify the minima or maxima\n",
    "might not be easy!\n",
    "But fret not, that's where computers come into the picture.\n",
    "There is a general purpose way of finding minima or maxima analytically,\n",
    "and that is by gradient descent.\n",
    "\n",
    "The core of gradient descent is as follows:\n",
    "We pick a starting point at will -\n",
    "there are heuristics to pick good starting points,\n",
    "but for now, let's be satisfied with \"at random\".\n",
    "We take little steps in the _negative_ direction of the gradient.\n",
    "At each step, we evaluate the gradient\n",
    "and then take a small step in $x$ that lets us follow\n",
    "the negative direction of the gradient.\n",
    "We repeat this process over and over\n",
    "for a fixed number of steps\n",
    "(which needs to be configured beforehand).\n",
    "This routine is what we call _gradient descent_.\n",
    "\n",
    "<img src=\"../images/01-gradient-descent/gradient-descent-diagram.png\">\n",
    "\n",
    "Those of you who are attuned to the algorithm\n",
    "might see the following key knobs for gradient descent:\n",
    "\n",
    "- Where the initial point is.\n",
    "- How large of a step to take.\n",
    "- How many steps to take.\n",
    "\n",
    "We won't immediately address how to set these,\n",
    "but you should know that those knobs exist.\n",
    "At this point in the course,\n",
    "I'd ask that you accept the default values that we'll provide for you\n",
    "knowing that pragmatically speaking,\n",
    "they'll help you solve the exercises you encounter\n",
    "in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ae1be-f72a-4e66-8592-833650621a6c",
   "metadata": {},
   "source": [
    "### The workhorse powered by _automatic differentiation_\n",
    "\n",
    "Gradient descent is the workhorse of all of modern deep learning.\n",
    "Whenever we fit a deep learning model,\n",
    "we are also minimizing a function -- \n",
    "the _loss function_ -- \n",
    "that tells us how bad our model is.\n",
    "We'll visit the idea of a loss function in the second notebook,\n",
    "so keep that name in your mind for the time being.\n",
    "For now, though, in order to help you get familiar with gradient descent\n",
    "and how to write a program that uses gradient descent in it,\n",
    "we're going to introduce to you an _automatic differentiation system_:\n",
    "a program that can return the derivative function\n",
    "of any function that performs numerical computation.\n",
    "We'll be using JAX,\n",
    "which provides automatic differentiation on top of the NumPy API --\n",
    "an API that many of us in the scientific Python world should be familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e1b66-802f-4eed-8734-fba97edb342e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## JAX's `grad` function\n",
    "\n",
    "Remember, when we are training a model to do some prediction,\n",
    "we need a way of calculating gradient functions so that we can do gradient-based optimization.\n",
    "Calculating the gradient function by hand is sometimes easy,\n",
    "especially if you're competent with algebra and calculus.\n",
    "However, in many cases, calculating the gradient function by hand is tedious and repetitive.\n",
    "As it turns out, though, tedious and repetitive tasks are _exactly_ what computers are great at!\n",
    "\n",
    "This is where JAX comes in.\n",
    "JAX is a Python package developed by friends at Google.\n",
    "You can think of JAX as the NumPy API turned on steroids. \n",
    "JAX ships with an automatic differentiation system,\n",
    "exposed to end-users in a simple function: `grad`.\n",
    "\n",
    "Here's what `grad` does.\n",
    "It takes in a function that returns a scalar output,\n",
    "and returns another _transformed_ function.\n",
    "That newly returned function is the gradient function of the original.\n",
    "The gradient function takes in the same arguments as the original,\n",
    "but will evaluate the gradient of the original function w.r.t. the arguments passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6a501-f36b-42b2-b376-a6b81688200b",
   "metadata": {},
   "source": [
    "### Example use of `grad`\n",
    "\n",
    "Let's see a concrete example of this in action.\n",
    "\n",
    "We have here an $x^2$ polynomial function:\n",
    "\n",
    "$$ f(x) = 3x^2 + 4x - 2 $$\n",
    "\n",
    "Implemented in NumPy, it'll look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46314139-cc1c-4a33-847b-43b43cddff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return 3 * x**2 + 4 * x - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316be66-aaab-48ec-b266-0eca0b223b34",
   "metadata": {},
   "source": [
    "Analytically, `func`'s gradient function is going to be:\n",
    "\n",
    "$$ f'(x) = 6x + 4 $$\n",
    "\n",
    "If we wanted, we could write that gradient function in NumPy by hand.\n",
    "However, we're instead going to obtain the gradient function using JAX's `grad` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05554213-be3e-4cef-a457-379d7ed7356e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "func_p = grad(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef7855-2c84-4020-bb18-f9c698835837",
   "metadata": {},
   "source": [
    "Pictorially, we'd represent the transformation as follows:\n",
    "\n",
    "<img src=\"../images/01-gradient-descent/grad-transform.png\">\n",
    "\n",
    "Here, `grad` acts as a program transformation device\n",
    "that takes `f` and turns it into `f'`, a.k.a. the first-order derivative of `f`.\n",
    "(You can keep passing the result through `grad` if you desire!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7814bb22-ae6a-4e32-8e1b-b7ce631eb56f",
   "metadata": {},
   "source": [
    "Now, we can evaluate the gradient function w.r.t. any value of $x$.\n",
    "Let's sanity-check that we have it right.\n",
    "If we plug in x = 3, then $f'(x) = 6(3) + 4 = 22$.\n",
    "Calling the same on `func_p`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64191b56-3d6a-4ccd-a796-b18d1ffc6a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "func_p(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c273f-71dc-43f9-bb8d-90dda40cf6d0",
   "metadata": {},
   "source": [
    "In the Jupyter notebook, I would encourage you to hand-calculate the gradient evaluated at various values of $x$ to convince yourself that `grad` does the right thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c9e08-85f4-453e-ace9-c40ae0985fa8",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Seeing how `grad` works isn't the main point though! We want to use the gradient function `func_p` in an optimization routine to _minimize_ the function `func`. Here's how we do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8309bbf-6962-4385-b1ac-31d7cddc0ae2",
   "metadata": {},
   "source": [
    "### Step 1: Initialize `x`\n",
    "\n",
    "Firstly, we start by initializing $x$ at some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d1a12-4fd8-4398-a601-1cb842cd4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e52901-cce7-44da-ada4-f2e13c57c13f",
   "metadata": {},
   "source": [
    "I chose a fixed value to start, but you could have just as well drawn a number from a random number generator instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d4d39-ceca-4a0f-923a-8196972d078b",
   "metadata": {},
   "source": [
    "### Step 2: Take small steps in negative direction of gradient\n",
    "\n",
    "Next, we take steps in the negative direction of the gradient a fixed number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f915f01-1d33-4e3d-840c-6f3e5ca4ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_steps = 300\n",
    "step_size = 5e-3\n",
    "\n",
    "# loop\n",
    "for i in range(num_steps):\n",
    "    x -= func_p(x) * step_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a1867-eca6-4ba6-8a6c-9fcd510e08d1",
   "metadata": {},
   "source": [
    "There are two _hyperparameters_ that we can configure here - the number of steps and the step size. Step sizes are canonically smaller than 1.0, and most of the time set by default to 0.005. (There's an Andrej Karpathy tweet about this.) The smaller the step size, the more steps may be needed to reach convergence. The number of steps is the other hyperparameter; the longer we run, the more likely we are to converge on the minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31be86a-e775-4980-a116-0c2d6c7fc768",
   "metadata": {},
   "source": [
    "### Step 3: Verify proximity to stationary point\n",
    "\n",
    "Let's make sure that we have hit someplace near a _stationary point_ in our optimization routine. Stationary points refer to places where the derivative of the function, or the evaluated gradient, is equal to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a9099-e381-4a57-9ab4-3865635f6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d62dc-7d00-4a95-a5ec-2ad198bf57f6",
   "metadata": {},
   "source": [
    "Indeed, we have!\n",
    "And that's a great sign that we're doing the right thing _computationally_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c75ee3-b5be-43de-b007-cd1a65e5078c",
   "metadata": {},
   "source": [
    "### Step 4: Check second derivative\n",
    "\n",
    "Let's also evaluate the second derivative to sanity-check that we have hit a minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b12aa-dfc5-4dba-ba5f-5a951102b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_pp = grad(func_p)\n",
    "func_pp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec46053-37a5-4caf-ba09-03ed2e2dc037",
   "metadata": {},
   "source": [
    "And just like that, we see that the second derivative at our final value of `x` is a positive number,\n",
    "so we know we're in a regime which is a minima!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e43bec-163b-42b8-a628-1f7d653d14d5",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Coming up is a pair of exercises for you to practice what you should have learned here today:\n",
    "\n",
    "1. How to obtain the derivative function of a NumPy/Python function, and\n",
    "2. How to use the evaluated derivative to find the minima of that function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2392b48-b841-4936-a86e-6ea976be777a",
   "metadata": {},
   "source": [
    "### Exercise: Obtaining the derivative function of a Python function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af1631-7f7c-42b8-908a-df21f06c5fef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise: Finding the minima of a Python function using derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef66891-1d5a-491f-b6c5-9c67f5f758c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
