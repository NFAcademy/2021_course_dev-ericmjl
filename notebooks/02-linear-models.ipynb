{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b82c66-dc02-4d33-987b-9867a41164f3",
   "metadata": {},
   "source": [
    "# Model, Loss and Optimizer for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda8907-cca0-4560-9d21-aaf9598f8197",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "You've made it pretty far now! \n",
    "I also hope you're excited to learn more!\n",
    "\n",
    "In the previous chapter, we've shown you this function called `grad`.\n",
    "`grad` is one of the key functions that JAX provides -\n",
    "it is the user-facing interface to\n",
    "the automatic differentiation system that JAX provides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82aed3-9539-49e3-9fce-9f3f00bbca5d",
   "metadata": {},
   "source": [
    "In this chapter, we're going to talk about linear regression.\n",
    "But wait, isn't this a course on deep learning fundamentals?\n",
    "Why are we talking about simple linear models?\n",
    "Well... it's because the _fundamental_ ideas that we need to learn\n",
    "apply the same way to linear models as they do to deep neural network models.\n",
    "As such, we're going to be _leveraging_ linear regression - \n",
    "at least for me, it serves as a minimally complex example \n",
    "to study the foundational ideas that generalize to deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a202851-9039-4c0b-b9ea-a934204dad4a",
   "metadata": {},
   "source": [
    "## What is a linear model?\n",
    "\n",
    "Your canonical linear model is something you've probably been taught in high school mathematics or physics.\n",
    "At its simplest form, the model expresses the idea that \"as some value increases, some other value should increase too\".\n",
    "In equation form, the linear model looks something like this:\n",
    "\n",
    "$$y = mx + c$$\n",
    "\n",
    "Linear models are popular because of their simple inductive bias - \n",
    "a simple Y-increases-with-X kind of relationship.\n",
    "We know that most of the world isn't linear,\n",
    "but if you zoom in hard enough into the world,\n",
    "you can make any pair of relationships look linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c7191-a7c4-4ccf-b571-b6bb0a55f70c",
   "metadata": {},
   "source": [
    "### Exercise: Implement linear model\n",
    "\n",
    "Let's build your intuition up by writing some Python code.\n",
    "We'll start by writing the linear model as a Python function.\n",
    "Implement the linear model below \n",
    "according to the following parameter specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037b0ac-cc2f-44d4-92b3-4b9eb302d529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear(params: tuple, x: float) -> float:\n",
    "    \"\"\"Univariate linear model.\n",
    "\n",
    "    :param params: A tuple of two scalar floats.\n",
    "        The first element should be the weight/slope\n",
    "        and the second element should be the bias/intercept.\n",
    "    :param x: Input data which can be scalar\n",
    "        or a vector of numbers.\n",
    "    :returns: A scalar of vector of outputs, y_estimated.\n",
    "    \"\"\"\n",
    "    # Your answer goes here\n",
    "    weight, bias = params \n",
    "    return weight * x + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fed1fe-8484-4c0b-baa5-f401cb5830bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How does optimization relate to linear models?\n",
    "\n",
    "The long answer cut short is this:\n",
    "_Whenever we train a linear model._\n",
    "(By the way, _training_ and _fitting_ a linear model are synonymous!)\n",
    "Gradient-based optimization comes into the picture\n",
    "as one of a few canonical ways to fit linear models to data.\n",
    "(There are exact analytical solutions, but we won't be covering them here\n",
    "because they would distract from our goal\n",
    "of seeing how gradient-based optimization\n",
    "is a general-purpose tool for fitting models.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a8857-29fa-4405-bb04-e1e8854966e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's break this down step-by-step.\n",
    "\n",
    "Firstly, we formulate the problem.\n",
    "If we squint hard at the linear model equation above,\n",
    "then we'll see that the _parameters_ of the model\n",
    "are $m$ and $c$, the slope and intercept parameters.\n",
    "(In the deep learning world,\n",
    "they are also referred to as weights and biases.)\n",
    "We would like to tweak their values\n",
    "such that when we evaluate the model's predictions\n",
    "against a ground truth dataset of $y_{\\text{true}}$ values\n",
    "given a bunch of $x$ values,\n",
    "we are able to obtain $y_{\\text{estimated}}$ values from the model\n",
    "that _minimizes the error_ between the $y_{\\text{estimated}}$ and $y_{\\text{true}}$ values.\n",
    "\n",
    "That last statement is the key!\n",
    "Remember that in any optimization problem\n",
    "our goal is to tweak parameters of something\n",
    "to minimize or maximize some other thing.\n",
    "When we train a linear model\n",
    "we are optimizing, or minimizing, an _error function_.\n",
    "In the deep learning literature,\n",
    "you might see this show up as the _loss function_,\n",
    "while in other optimization literature circles,\n",
    "you might see this show up as the _cost function_.\n",
    "Those are synonyms for one another.\n",
    "\n",
    "What, then, is a reasonable linear model loss function?\n",
    "This is something we get to design!\n",
    "A common loss function for linear models\n",
    "_that produce continuous outputs_\n",
    "is the mean squared error (MSE) function.\n",
    "As its name suggests,\n",
    "it is the mean of the squares of the errors between our model-predicted and actual values.\n",
    "In math form, it looks like the following:\n",
    "\n",
    "$$\\text{MSE}(\\text{params}, x, y) = \\frac{\\sum_{i}^{n} ({y_{i} - f(\\text{params}, x_{i})})^2}{n}$$\n",
    "\n",
    "Let's see what the intuition behind the MSE is...\n",
    "without going into a full derivation of why the MSE function is a good loss function.\n",
    "The numerator of the function above is the sum of squared errors.\n",
    "In a good model,\n",
    "we might believe that errors might be distributed more or less evenly\n",
    "above and below the line of best fit (given by the tweaked values of $m$ and $c$).\n",
    "As such, there'll be some positive and some negative values of errors.\n",
    "Now, it's kind of nice to work with just the error values raw,\n",
    "but if we would like a smiley-faced function that we can optimize,\n",
    "taking squares is a good idea. \n",
    "Finally, we take an average over all of the $n$ data points that are present in the dataset - \n",
    "kind of useful if, for example, we want to compare the fit of the same class of model\n",
    "on two datasets with different sizes.\n",
    "\n",
    "**Note:** This next point has to be clarified:\n",
    "we are _not_ optimizing the linear model function.\n",
    "Rather, we are optimizing the parameters of the linear model\n",
    "to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfa06d-5451-4bf7-b824-f45a917a123d",
   "metadata": {},
   "source": [
    "### Exercise: Implement loss function\n",
    "\n",
    "Now, let's implement the mean squared error loss function as a Python + NumPy function.\n",
    "Implement the function according to the Python specs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054858e-7508-491a-b3db-ec7f3c238304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import jax.numpy as np\n",
    "\n",
    "def mse_loss(params: tuple, model: Callable, x: float, y: float):\n",
    "    \"\"\"\n",
    "    Mean squared error loss function.\n",
    "\n",
    "    :param params: A tuple of params that gets passed into the `model` function.\n",
    "    :param model: A Python callable that accepts `params` and `x` and returns `y_estimated`.\n",
    "    :param x: Measured values of `x`.\n",
    "    :param y: Measured values of `y`.\n",
    "    \"\"\"\n",
    "    y_est = model(params, x)\n",
    "    sq_err = np.power(y_est - y, 2)\n",
    "    return np.mean(sq_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b021ea93-3bf6-44eb-a7a0-19cf259e7db3",
   "metadata": {},
   "source": [
    "## Obtaining the gradient of the MSE loss function w.r.t. linear model parameters\n",
    "\n",
    "What's coming up is going to hopefully illustrate for you \n",
    "why automatic differentiation systems are so powerful.\n",
    "We now have a loss function that we're going to minimize for a given model.\n",
    "If we wish to use gradient-based optimization to minimize the loss function,\n",
    "then we're going to need to obtain its gradients w.r.t. model parameters.\n",
    "I'm going to put it to you that:\n",
    "\n",
    "1. the calculus involved is tedious, and\n",
    "2. if we change the model structure, which will definitely happen as we progress to neural networks, re-computing gradient functions by hand is also incredibly annoying.\n",
    "\n",
    "That, then, is why having an automatic differentiation system is so powerful.\n",
    "With the `grad` function in JAX,\n",
    "or any automatic differentiation system implemented in other deep learning libraries,\n",
    "you get the benefit of being able to obtain gradient functions\n",
    "without necessarily needing to do any of that by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1887fa3-6fb7-469d-b7f1-818627f99d92",
   "metadata": {},
   "source": [
    "### Exercise: Obtain gradient function of the MSE loss function\n",
    "\n",
    "Now, I'd like you to obtain the gradient function of the MSE loss function.\n",
    "This should literally be a one-liner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d7518-c442-4eac-8ccd-1a65a90f688e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "dmse_loss = grad(mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f48be4-df55-4281-91dc-b68e17ecc00d",
   "metadata": {},
   "source": [
    "## Optimize the linear model parameters\n",
    "\n",
    "With the gradient function on-hand, \n",
    "we can now use gradient-based optimization to optimize the parameters $m$ and $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3787b-2208-4265-af32-afd085482343",
   "metadata": {},
   "source": [
    "### Exercise: Implement optimization loop\n",
    "\n",
    "Your final exercise here is to implement the optimization loop.\n",
    "I have provided you with a dataset of values\n",
    "that are obtained from a linear model.\n",
    "By implementing the optimization loop,\n",
    "your goal is to figure out what the values of the linear model parameters $m$ and $c$ are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72321118-fdf3-44a6-bdf9-d39cccc069ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(49)\n",
    "\n",
    "x = random.normal(key, shape=(1000,))\n",
    "y = 3.5 * x - 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8596c1-e6d1-4c43-ba04-2d6ad0624420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Firstly, we have initialized guesses for params.\n",
    "\n",
    "k1, k2 = random.split(key)\n",
    "params = random.normal(k1, shape=(2,))\n",
    "\n",
    "# Now, write the optimization loop.\n",
    "\n",
    "for i in range(200):\n",
    "    # THIS IS WHERE THE EXERCISE LIVES!\n",
    "    grad_params = dmse_loss(params, linear, x, y)\n",
    "    params -= grad_params * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb13e3-5953-4eb8-9d0a-c08abbdaffb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeec5a4-4999-490a-be31-33d7b8e4f401",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07704216-1b30-4643-be05-9a5b2a3b4892",
   "metadata": {},
   "source": [
    "### The pattern\n",
    "\n",
    "In the first chapter, we showed you how to optimize a simple polynomial function of $x$.\n",
    "The key steps were as follows:\n",
    "\n",
    "1. Define the function of $x$ in Python code using NumPy.\n",
    "2. Obtain its corresponding gradient function using `grad`\n",
    "3. Use the evaluated gradient in a so-called _training loop_ to obtain the value of $x$ that minimizes the function.\n",
    "\n",
    "With training linear models, we have an analogous situation:\n",
    "\n",
    "1. Firstly, we define the mean squared error in terms of the key parameters we want to optimize, $m$ and $c$.\n",
    "2. Obtain its corresponding gradient function using `grad`.\n",
    "3. Use a _training loop_ to iteratively optimize $m$ and $c$ such that it minimizes the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7096d3-6685-464f-8f06-fa07c7f1a97f",
   "metadata": {},
   "source": [
    "### Automatic differentiation (AD)\n",
    "\n",
    "I also wanted to make a point about automatic differentiation systems once more.\n",
    "Without an automatic differentiation system, you would have had to implement the gradient function by hand.\n",
    "With simple functions like polynomials,\n",
    "that's not too tedious,\n",
    "but once you go into the realm of derivatives for a wide variety of models,\n",
    "taking derivatives over and over becomes a chore, tedium more suited to computers than humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff4148-6a32-485a-ab99-536da3cf4fbe",
   "metadata": {},
   "source": [
    "### Looking forward\n",
    "\n",
    "In the next two chapters, we will be swapping out the linear model for other models.\n",
    "There, the power of automatic differentiation will become even more evident!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5ecfe-3b4f-4147-b27b-1d2461271ef9",
   "metadata": {},
   "source": [
    "## Final exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1b9b9-8a39-4949-8e44-6efcb4cc69db",
   "metadata": {},
   "source": [
    "### Exercise: When it comes to _regression_ problems, what loss function is commonly used?\n",
    "\n",
    "- [x] Mean squared error\n",
    "- [ ] Mean regression error\n",
    "- [ ] Mean simple error\n",
    "- [ ] Median squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb2c1b-a6f7-4ef3-bc47-5d0f178ad07a",
   "metadata": {},
   "source": [
    "### Exercise: In linear regression, what is the mean squared error being optimized with respect to?\n",
    "\n",
    "- [ ] The output `y`\n",
    "- [ ] The input `x` and parameters `m` and `c`\n",
    "- [x] The parameters `m` and `c`\n",
    "- [ ] The inputs `x` and the outputs `y`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
